{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vassi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download the 'train.csv' and 'test.csv' files, follow the link: https://drive.google.com/drive/folders/1GtFMaqWYieUR9A6D3gO_u8QjsBVkIi8H?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'C:/Users/vassi/Desktop/'\n",
    "train = pd.read_csv(location+'train.csv')\n",
    "test = pd.read_csv(location+'test.csv')\n",
    "train = train[~((train['toxic']==0) & (train['severe_toxic']==0) & (train['obscene']==0) & (train['threat']==0) & (train['insult']==0) & (train['identity_hate']==0))]\n",
    "train = train.reset_index().drop(['index', 'id'], axis=1)\n",
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    sw_nltk = stopwords.words('english')\n",
    "    sw_nltk.append('st')\n",
    "    sw_nltk.append('nd')\n",
    "    sw_nltk.append('rd')\n",
    "    sw_nltk.append('th')\n",
    "    sw_nltk.append('rt')\n",
    "    words = [word for word in text.split() if word.lower() not in sw_nltk]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = ''.join(i for i in text if not i.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete nan messages and clean the messages\n",
    "\n",
    "train=train[~train['comment_text'].isna()]\n",
    "train['comment_text']=train['comment_text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "#remove empty comments and stopwords\n",
    "train = train[~train['comment_text'].isin(['', ' ', np.nan, None, []])]\n",
    "train['comment_text']=train['comment_text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#remove empty comments after the removal of stopwords\n",
    "train = train[~train['comment_text'].isin(['', ' ', np.nan, None, []])]\n",
    "# df.to_csv(location+'Chat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(train, random_state=420, test_size=0.2, shuffle=True)\n",
    "y_train = X_train.drop(['comment_text'],axis=1)\n",
    "X_train = X_train.comment_text\n",
    "y_test = X_test.drop(['comment_text'],axis=1)\n",
    "X_test = X_test.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9405238828967643\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.9035439137134053\n",
      "... Processing obscene\n",
      "Test accuracy is 0.737442218798151\n",
      "... Processing threat\n",
      "Test accuracy is 0.9694915254237289\n",
      "... Processing insult\n",
      "Test accuracy is 0.6807395993836671\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9124807395993837\n"
     ]
    }
   ],
   "source": [
    "# Define a pipeline combining a text feature extractor with multi lable classifier\n",
    "\n",
    "df_toxic = train.drop(['comment_text'], axis=1)\n",
    "counts = []\n",
    "categories = list(df_toxic.columns.values)\n",
    "\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    NB_pipeline.fit(X_train, y_train[category])\n",
    "    # compute the testing accuracy\n",
    "    prediction = NB_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[category], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9380585516178737\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.8970724191063174\n",
      "... Processing obscene\n",
      "Test accuracy is 0.7895223420647149\n",
      "... Processing threat\n",
      "Test accuracy is 0.9738058551617874\n",
      "... Processing insult\n",
      "Test accuracy is 0.6844375963020031\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.92326656394453\n"
     ]
    }
   ],
   "source": [
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    SVC_pipeline.fit(X_train, y_train[category])\n",
    "    # compute the testing accuracy\n",
    "    prediction = SVC_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[category], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Test accuracy is 0.9405238828967643\n",
      "... Processing severe_toxic\n",
      "Test accuracy is 0.903235747303544\n",
      "... Processing obscene\n",
      "Test accuracy is 0.8024653312788906\n",
      "... Processing threat\n",
      "Test accuracy is 0.9725731895223421\n",
      "... Processing insult\n",
      "Test accuracy is 0.711864406779661\n",
      "... Processing identity_hate\n",
      "Test accuracy is 0.9198767334360555\n"
     ]
    }
   ],
   "source": [
    "LogReg_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    LogReg_pipeline.fit(X_train, y_train[category])\n",
    "    # compute the testing accuracy\n",
    "    prediction = LogReg_pipeline.predict(X_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[category], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
